{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Kernel Method challenge**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Initialization and data preparation\n",
    "\n",
    "* Data are loaded and standardized with zero mean and unit variance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Load and standardize features by removing the mean and scaling to unit variance\n",
    "StandardScaler = lambda df: (df-df.mean())/df.std()\n",
    "\n",
    "XTrain0 = pd.read_csv('./data/Xtr0_mat100.csv', sep=' ', header=None)\n",
    "XTrain1 = pd.read_csv('./data/Xtr1_mat100.csv', sep=' ', header=None)\n",
    "XTrain2 = pd.read_csv('./data/Xtr2_mat100.csv', sep=' ', header=None)\n",
    "\n",
    "\n",
    "XTest0 = (pd.read_csv('./data/Xte0_mat100.csv', sep=' ', header=None)-XTrain0.mean())/XTrain0.std()\n",
    "XTest1 = (pd.read_csv('./data/Xte1_mat100.csv', sep=' ', header=None)-XTrain1.mean())/XTrain1.std()\n",
    "XTest2 = (pd.read_csv('./data/Xte2_mat100.csv', sep=' ', header=None)-XTrain2.mean())/XTrain2.std()\n",
    "\n",
    "YTrain0 = pd.read_csv('./data/Ytr0.csv', usecols = ['Bound'])\n",
    "YTrain1 = pd.read_csv('./data/Ytr1.csv', usecols = ['Bound'])\n",
    "YTrain2 = pd.read_csv('./data/Ytr2.csv', usecols = ['Bound'])\n",
    "\n",
    "\n",
    "XTrain0 = StandardScaler(XTrain0)\n",
    "XTrain1 = StandardScaler(XTrain1)\n",
    "XTrain2 = StandardScaler(XTrain2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Baseline Model: Ridge Regression on the numerical features (from scratch)\n",
    "\n",
    "\n",
    "This section contains:\n",
    "\n",
    "* The implementation of the Ridge Regression from scratch\n",
    "* A comparison of the results with the one provided by the sklearn implementation. Such test is performed using part of the training set for training and the rest for testing and to compute performances accuracy. \n",
    "\n",
    "\n",
    "* https://github.com/akaashagarwal/ridge-regression "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class RidgeRegression:\n",
    "    \"\"\"Class to fit a ridge regression model on a given training set, and make predictions on data.\n",
    "    Specifically, the cost function with L_2 regularization is assumed to be:\n",
    "        ||w - Xw||^2_2 + alpha * ||w||2_2\n",
    "    where,\n",
    "        w = Model weights,\n",
    "        X = Data design matrix,\n",
    "        alpha = regularization parameter.\n",
    "    The class uses batch gradient descent to optimize the model weights.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, learning_rate: float, reg_strength: float, max_iter: int) -> None:\n",
    "        \"\"\"Initialize.\n",
    "        Args:\n",
    "            learning_rate (float): Learning rate for gradient descent.\n",
    "            reg_strength (float): Regularization parameter, to control the bias-variance tradeoff.\n",
    "            max_iter (int): Number of iterations to run gradient descent.\n",
    "        \"\"\"\n",
    "        self.learn_rate = learning_rate\n",
    "        self.reg_strength = reg_strength\n",
    "        self.max_iter = max_iter\n",
    "        self._weights: np.array\n",
    "        self.design_matrix: np.array\n",
    "        self.y_train: np.array\n",
    "        self.train_size: int\n",
    "        self.num_feats: int\n",
    "\n",
    "    @property\n",
    "    def weights(self):\n",
    "        \"\"\"Return ridge regression model weights.\"\"\"\n",
    "        return self._weights\n",
    "\n",
    "    def fit(self, x_train, y_train) -> None:\n",
    "        \"\"\"Fit model weights to training data.\"\"\"\n",
    "        self.train_size, self.num_feats = x_train.shape[0], x_train.shape[1]\n",
    "        self.design_matrix = np.append(np.ones(self.train_size).reshape(\n",
    "            -1, 1), x_train, axis=1)  # Add one for the intercept term for all training examples.\n",
    "        self.y_train = y_train.to_numpy()\n",
    "        self._weights = np.zeros(self.num_feats + 1)  # +1 for the intercept.\n",
    "\n",
    "        self.optimize_weights()\n",
    "\n",
    "    def optimize_weights(self) -> None:\n",
    "        \"\"\"Optimize model weights for self.max_iter iterations.\"\"\"\n",
    "        for _ in range(self.max_iter):\n",
    "            self.update_step()\n",
    "\n",
    "    def update_step(self) -> None:\n",
    "        \"\"\"\n",
    "            Update model weights with batch gradient descent step.\n",
    "            Recall, the update statement for a model weight theta_k is:\n",
    "                theta_k = theta_k - (learn_rate * J_theta_k)\n",
    "                where J_theta_k = cost function\n",
    "                      J_theta_k = (2 / train_size) * ( ((y_hat - y_real) * x_k) + (alpha * theta_k^2))\n",
    "        \"\"\"\n",
    "        y_hat = (self._weights * self.design_matrix).sum(axis=1)\n",
    "        errors = (y_hat - self.y_train).reshape(-1, 1)\n",
    "\n",
    "        j_theta = (2 / self.train_size) * ((errors * self.design_matrix).sum(axis=0) +\n",
    "                                           (self.reg_strength * self._weights))\n",
    "        step = self.learn_rate * j_theta\n",
    "\n",
    "        self._weights = self._weights - step.reshape(-1)\n",
    "\n",
    "    def predict(self, x_test):\n",
    "        \"\"\"Make predictions for x_test using trained model.\n",
    "        Args:\n",
    "            x_test (np.array): Input test data set.\n",
    "        Returns:\n",
    "            np.array: Predictions for x_test.\n",
    "        \"\"\"\n",
    "        test_size = x_test.shape[0]\n",
    "        x_test = np.append(np.ones(test_size).reshape(-1, 1), x_test, axis=1)\n",
    "\n",
    "        return (self._weights * x_test).sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RR from scratch: Error on the 5% of the training set: 0.38\n",
      "Sklearn implementation: Error on the 5% of the training set: 0.38\n"
     ]
    }
   ],
   "source": [
    "# Test on the training data of the Ridge Regression from scratch \n",
    "RidgeRegressor = RidgeRegression(learning_rate=0.1, reg_strength=10, max_iter=1000)\n",
    "RidgeRegressor.fit(XTrain0[:1900], YTrain0['Bound'][:1900])\n",
    "predictions = RidgeRegressor.predict(XTrain0[1900:])\n",
    "y_true = np.array(YTrain0['Bound'][1900:])\n",
    "y_pred = np.array(np.rint(y_pred))\n",
    "abs_errors = np.abs((y_true - y_pred))\n",
    "print('RR from scratch: Error on the 5% of the training set: {}'.format(np.mean(abs_errors)))\n",
    "\n",
    "from sklearn.linear_model import Ridge\n",
    "sklearnRegressor = Ridge(alpha=10, solver='sag', max_iter=1000)\n",
    "model = sklearnRegressor.fit(XTrain0[:1900], YTrain0['Bound'][:1900])\n",
    "predictions = model.predict(XTrain0[1900:])\n",
    "\n",
    "y_true = np.array(YTrain0['Bound'][1900:])\n",
    "y_pred = np.array(np.rint(y_pred))\n",
    "\n",
    "abs_errors = np.abs((y_true - y_pred))\n",
    "print('Sklearn implementation: Error on the 5% of the training set: {}'.format(np.mean(abs_errors)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# First submission using Ridge Regression\n",
    "\n",
    "RidgeRegressor = RidgeRegression(learning_rate=0.1, reg_strength=10, max_iter=1000)\n",
    "RidgeRegressor.fit(XTrain0, YTrain0['Bound'])\n",
    "predictions0 = RidgeRegressor.predict(XTest0)\n",
    "predictions0 = np.rint(predictions0).astype(int)\n",
    "\n",
    "df0 = pd.DataFrame({'Id': np.arange(1000), 'Bound': predictions0})\n",
    "\n",
    "RidgeRegressor = RidgeRegression(learning_rate=0.1, reg_strength=10, max_iter=1000)\n",
    "RidgeRegressor.fit(XTrain1, YTrain1['Bound'])\n",
    "predictions1 = RidgeRegressor.predict(XTest1)\n",
    "predictions1 = np.rint(predictions1).astype(int)\n",
    "\n",
    "df1 = pd.DataFrame({'Id': np.arange(1000,2000), 'Bound': predictions1})\n",
    "\n",
    "\n",
    "RidgeRegressor = RidgeRegression(learning_rate=0.1, reg_strength=10, max_iter=1000)\n",
    "RidgeRegressor.fit(XTrain2, YTrain2['Bound'])\n",
    "predictions2 = RidgeRegressor.predict(XTest2)\n",
    "predictions2 = np.rint(predictions2).astype(int)\n",
    "\n",
    "df2 = pd.DataFrame({'Id': np.arange(2000,3000), 'Bound': predictions2})\n",
    "\n",
    "\n",
    "dfResult = pd.concat([df0,df1,df2])\n",
    "dfResult.to_csv('./data/submissionRidgeRegression.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Kernel Ridge Regression on the numerical features\n",
    "\n",
    "This section contains:\n",
    "\n",
    "* The implementation of the Kernel Ridge Regression from scratch, with the generation of the Kaggle submission file. \n",
    "* A coarse grid searches on the parameters (the regularisation parameter $\\lambda$ and $\\sigma$ the std of the gaussian Kernel)\n",
    "* A comparison of the results with the one provided by the sklearn implementation. Such test is performed using part of the training set for training and the rest for testing and to compute performances accuracy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "code_folding": [
     5
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Kernel Ridge regression and creation of the Kaggle submission file\n",
    "\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "\n",
    "def gaussian_kernel(z):\n",
    "    return (1/np.sqrt(2*np.pi))*np.exp(-0.5*z**2)\n",
    "\n",
    "def distance_matrix(A, B, squared=False):\n",
    "    \"\"\"\n",
    "    Compute all pairwise distances between vectors in A and B.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    A : np.array\n",
    "        shape should be (M, K)\n",
    "    B : np.array\n",
    "        shape should be (N, K)\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    D : np.array\n",
    "        A matrix D of shape (M, N).  Each entry in D i,j represnets the\n",
    "        distance between row i in A and row j in B.\n",
    "\n",
    "    See also\n",
    "    --------\n",
    "    A more generalized version of the distance matrix is available from\n",
    "    scipy (https://www.scipy.org) using scipy.spatial.distance_matrix,\n",
    "    which also gives a choice for p-norm.\n",
    "    \"\"\"\n",
    "    M = A.shape[0]\n",
    "    N = B.shape[0]\n",
    "\n",
    "    assert A.shape[1] == B.shape[1], f\"The number of components for vectors in A \\\n",
    "        {A.shape[1]} does not match that of B {B.shape[1]}!\"\n",
    "\n",
    "    A_dots = (A*A).sum(axis=1).reshape((M,1))*np.ones(shape=(1,N))\n",
    "    B_dots = (B*B).sum(axis=1)*np.ones(shape=(M,1))\n",
    "    D_squared =  A_dots + B_dots -2*A.dot(B.T)\n",
    "\n",
    "    if squared == False:\n",
    "        zero_mask = np.less(D_squared, 0.0)\n",
    "        D_squared[zero_mask] = 0.0\n",
    "        return np.sqrt(D_squared)\n",
    "\n",
    "    return D_squared\n",
    "    \n",
    "    \n",
    "sigma = 1.233\n",
    "\n",
    "# Computation of the three Kernel Matrix Ki\n",
    "K0 = gaussian_kernel(distance_matrix(XTrain0.to_numpy(), XTrain0.to_numpy())/sigma)\n",
    "K1 = gaussian_kernel(distance_matrix(XTrain1.to_numpy(), XTrain1.to_numpy())/sigma)\n",
    "K2 = gaussian_kernel(distance_matrix(XTrain2.to_numpy(), XTrain2.to_numpy())/sigma)\n",
    "\n",
    "lambdaParameter=1\n",
    "\n",
    "n = XTrain0.shape[0]\n",
    "alpha0 = np.linalg.solve(K0+lambdaParameter*n*np.eye(XTrain0.shape[0]),2*YTrain0.to_numpy()-1)\n",
    "\n",
    "n = XTrain1.shape[0]\n",
    "alpha1 = np.linalg.solve(K1+lambdaParameter*n*np.eye(XTrain1.shape[0]),2*YTrain1.to_numpy()-1)\n",
    "\n",
    "n = XTrain2.shape[0]\n",
    "alpha2 = np.linalg.solve(K2+lambdaParameter*n*np.eye(XTrain2.shape[0]),2*YTrain2.to_numpy()-1)\n",
    "\n",
    "# Predictions using the previsously computed weights (i.e the optimal function ion the RKHS)\n",
    "predictions0 = alpha0.T@gaussian_kernel(distance_matrix(XTrain0.to_numpy(), XTest0.to_numpy())/sigma)\n",
    "predictions1 = alpha1.T@gaussian_kernel(distance_matrix(XTrain1.to_numpy(), XTest1.to_numpy())/sigma)\n",
    "predictions2 = alpha2.T@gaussian_kernel(distance_matrix(XTrain2.to_numpy(), XTest2.to_numpy())/sigma)\n",
    "\n",
    "# Predictions where -1 or +1, we set them back to 0, 1 for the submission.\n",
    "predictions0 = (1+np.sign(predictions0))/2\n",
    "predictions1 = (1+np.sign(predictions1))/2\n",
    "predictions2 = (1+np.sign(predictions2))/2\n",
    "\n",
    "# Creation of the Kaggle submission file\n",
    "df0 = pd.DataFrame({'Id': np.arange(1000), 'Bound': predictions0.squeeze().astype(int)})\n",
    "df1 = pd.DataFrame({'Id': np.arange(1000,2000), 'Bound': predictions1.squeeze().astype(int)})\n",
    "df2 = pd.DataFrame({'Id': np.arange(2000,3000), 'Bound': predictions2.squeeze().astype(int)})\n",
    "dfResult = pd.concat([df0,df1,df2])\n",
    "dfResult.to_csv('./data/submissionKernelRidgeRegression.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kernel Ridge Regression from scratch implementation: Error on the 5% of the training set: 0.385\n",
      "Sklearn implementation: Error on the 5% of the training set: 0.385\n"
     ]
    }
   ],
   "source": [
    "# Comparison of our implementation and the Sklearn one \n",
    "\n",
    "def gaussian_kernel(z):\n",
    "    return (1/np.sqrt(2*np.pi))*np.exp(-0.5*z**2)\n",
    "\n",
    "# Computation of the three Kernel Matrix Ki\n",
    "K0 = gaussian_kernel(distance_matrix(XTrain0[:1800].to_numpy(), XTrain0[:1800].to_numpy())/sigma)\n",
    "\n",
    "lambdaParameter=1\n",
    "n = XTrain0[:1800].shape[0]\n",
    "alpha0 = np.linalg.solve(K0+lambdaParameter*n*np.eye(XTrain0[:1800].shape[0]),2*YTrain0[:1800].to_numpy()-1)\n",
    "\n",
    "# Predictions using the previsously computed weights (i.e the optimal function ion the RKHS)\n",
    "predictions0 = alpha0.T@gaussian_kernel(distance_matrix(XTrain0[:1800].to_numpy(), XTrain0[1800:].to_numpy())/sigma)\n",
    "\n",
    "y_true = np.array(YTrain0['Bound'][1800:])\n",
    "y_pred = (1+np.sign(predictions0))/2\n",
    "\n",
    "abs_errors = np.abs((y_true - y_pred))\n",
    "print('Kernel Ridge Regression from scratch implementation: Error on the 5% of the training set: {}'.format(np.mean(abs_errors)))\n",
    "\n",
    "\n",
    "# Comparison with the sklearn implementation \n",
    "from sklearn.kernel_ridge import KernelRidge\n",
    "def gaussian_kernel(x,y):\n",
    "    return (1/np.sqrt(2*np.pi))*np.exp(-0.5*np.linalg.norm(x-y)**2)\n",
    "    \n",
    "    \n",
    "clf = KernelRidge(alpha=1.0, kernel=gaussian_kernel)\n",
    "clf.fit(XTrain0[:1800], 2*YTrain0['Bound'][:1800]-1)\n",
    "predictions = clf.predict(XTrain0[1800:])\n",
    "y_true = np.array(YTrain0['Bound'][1800:])\n",
    "y_pred = (1+np.sign(predictions))/2\n",
    "\n",
    "abs_errors = np.abs((y_true - y_pred))\n",
    "print('Sklearn implementation: Error on the 5% of the training set: {}'.format(np.mean(abs_errors)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Grid search on the parameters of the Kernel Ridge Regression\n",
    "\n",
    "def gaussian_kernel(z):\n",
    "    return (1/np.sqrt(2*np.pi))*np.exp(-0.5*z**2)\n",
    "\n",
    "\n",
    "for sigm in np.linspace(1.1,1.3,10):#[0.4, 0.45, 0.5, 0.55, 0.6]:\n",
    "    for lambda_param in [0.1, 1., 10.]:\n",
    "        print('\\nLambda: {} sigma: {}'.format(lambda_param,sigm))\n",
    "\n",
    "        # Computation of the three Kernel Matrix Ki\n",
    "        K0 = gaussian_kernel(distance_matrix(XTrain0[:1800].to_numpy(), XTrain0[:1800].to_numpy())/sigm)\n",
    "        n = XTrain0[:1800].shape[0]\n",
    "        alpha0 = np.linalg.solve(K0+lambda_param*n*np.eye(XTrain0[:1800].shape[0]),2*YTrain0[:1800].to_numpy()-1)\n",
    "        # Predictions using the previsously computed weights (i.e the optimal function ion the RKHS)\n",
    "        predictions0 = alpha0.T@gaussian_kernel(distance_matrix(XTrain0[:1800].to_numpy(), XTrain0[1800:].to_numpy())/sigm)\n",
    "        y_true = np.array(YTrain0['Bound'][1800:])\n",
    "        y_pred = (1+np.sign(predictions0))/2\n",
    "        abs_errors0 = np.abs((y_true - y_pred))\n",
    "        #print('XTRAIN0: Error on the 5% of the training set: {}'.format(np.mean(abs_errors)))\n",
    "        \n",
    "        # Computation of the three Kernel Matrix Ki\n",
    "        K0 = gaussian_kernel(distance_matrix(XTrain1[:1800].to_numpy(), XTrain1[:1800].to_numpy())/sigm)\n",
    "        n = XTrain1[:1800].shape[0]\n",
    "        alpha0 = np.linalg.solve(K0+lambda_param*n*np.eye(XTrain1[:1800].shape[0]),2*YTrain1[:1800].to_numpy()-1)\n",
    "        # Predictions using the previsously computed weights (i.e the optimal function ion the RKHS)\n",
    "        predictions0 = alpha0.T@gaussian_kernel(distance_matrix(XTrain1[:1800].to_numpy(), XTrain1[1800:].to_numpy())/sigm)\n",
    "        y_true = np.array(YTrain1['Bound'][1800:])\n",
    "        y_pred = (1+np.sign(predictions0))/2\n",
    "        abs_errors1 = np.abs((y_true - y_pred))\n",
    "        #print('XTRAIN1: Error on the 5% of the training set: {}'.format(np.mean(abs_errors)))\n",
    "\n",
    "        # Computation of the three Kernel Matrix Ki\n",
    "        K0 = gaussian_kernel(distance_matrix(XTrain2[:1800].to_numpy(), XTrain2[:1800].to_numpy())/sigm)\n",
    "        n = XTrain1[:1800].shape[0]\n",
    "        alpha0 = np.linalg.solve(K0+lambda_param*n*np.eye(XTrain2[:1800].shape[0]),2*YTrain2[:1800].to_numpy()-1)\n",
    "        # Predictions using the previsously computed weights (i.e the optimal function ion the RKHS)\n",
    "        predictions0 = alpha0.T@gaussian_kernel(distance_matrix(XTrain2[:1800].to_numpy(), XTrain2[1800:].to_numpy())/sigm)\n",
    "        y_true = np.array(YTrain2['Bound'][1800:])\n",
    "        y_pred = (1+np.sign(predictions0))/2\n",
    "        abs_errors2 = np.abs((y_true - y_pred))\n",
    "        print('Mean error on the 5% of the 3 training set: {}'.format(np.mean(list(abs_errors0.squeeze())+list(abs_errors1.squeeze())+list(abs_errors2.squeeze()))))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Support Vector Machine "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3.7.5 64-bit ('base': conda)",
   "language": "python",
   "name": "python37564bitbasecondaa50c3607aaec4d41a1290a133b998fce"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": true,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
